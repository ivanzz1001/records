---
layout: post
title: bond模式详解与配置
tags:
- LinuxOps
categories: linuxOps
description: bond模式详解与配置
---

文章转载自:

- [bond模式详解与配置](https://www.cnblogs.com/Leonardo-li/p/18179341)

- [什么是以太网链路聚合](https://cloud.tencent.com/developer/article/2330886)

- [bonding技术](https://www.cnblogs.com/snnu/p/10293872.html)


<!-- more -->

## 1. bond介绍
以太网通道绑定（Ethernet Channel Bonding）是一种网络技术，用于将多个物理网络接口（通常是以太网网卡）组合成一个逻辑接口。这样做的目的是增加网络带宽、提高可靠性和提供冗余备份。

在以太网通道绑定中，多个物理接口被绑定在一起形成一个虚拟的接口，称为 Bond 接口。该 Bond 接口有自己的 IP 地址和 MAC 地址，它看起来就像是单个网络接口一样，但实际上是由多个物理接口组成的。Bond 接口上的数据包可以通过任何一个物理接口进行传输，并且可以根据配置的策略在多个物理接口之间分配。

以太网通道绑定有许多优点，包括：

- 增加带宽：通过将多个物理接口绑定在一起，可以增加可用的带宽。如果每个物理接口的带宽是相同的，则总带宽将成倍增加。

- 提高可靠性：当一个物理接口发生故障时，其他接口仍然可以继续工作，从而提高了网络的可靠性。这种冗余设计可以防止单点故障。

- 提供负载均衡：通过合理配置，可以将网络流量均匀地分布到各个物理接口上，从而实现负载均衡。这有助于最大程度地利用可用带宽，并防止某些接口过载。

- 故障自动恢复：一旦一个物理接口失效，系统可以自动将流量切换到其他可用的接口，从而实现快速的故障恢复。


以太网通道绑定在服务器环境中特别常见，用于连接到交换机或路由器的网络连接。它可以通过多种配置模式来适应不同的网络需求，例如负载均衡、冗余备份、动态链接聚合等。

## 2. bond模式说明
bonding技术提供了七种工作模式，在使用的时候需要指定一种，每种有各自的优缺点.

- balance-rr (mode=0) 默认, 有高可用 (容错) 和负载均衡的功能, 需要交换机的配置，每块网卡轮询发包 (流量分发比较均衡).

- active-backup (mode=1) 只有高可用 (容错) 功能, 不需要交换机配置, 这种模式只有一块网卡工作, 对外只有一个mac地址。缺点是端口利用率比较低

- balance-xor (mode=2) 不常用

- broadcast (mode=3) 不常用

- 802.3ad (mode=4) IEEE 802.3ad 动态链路聚合，需要交换机配置

- balance-tlb (mode=5) 不常用

- balance-alb (mode=6) 有高可用 ( 容错 )和负载均衡的功能，不需要交换机配置 (流量分发到每个接口不是特别均衡)。

## 3. bond配置

1) 配置信息（如果要更换模式的话，只需要在ifcfg-bind0网卡文件中修改对应模式，并配置好对应交换机模式即可）

{% highlight string %}
系统: Centos7.8
网卡: ifcfg-eno1、ifcfg-eno2
bond0：172.16.3.94
负载模式: mode1（主备）
{% endhighlight %}

2) 关闭和停止NetworkManager服务（一定要关闭，不关会对做bonding有干扰）
{% highlight string %}
systemctl stop NetworkManager.service     # 停止NetworkManager服务
systemctl disable NetworkManager.service  # 禁止开机启动NetworkManager服务
{% endhighlight %}

3) 加载bonding模块（没有提示说明加载成功）
{% highlight string %}
modprobe bonding
{% endhighlight %}

4) 查看bonding模块是否被加载
{% highlight string %}
[root@localhost ~]# lsmod | grep bonding
bonding               152979  0 
{% endhighlight %}

5) 配置内容
{% highlight string %}
#网卡eno1配置
[root@localhost network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-eno1 
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=eno1
UUID=6a9a57ce-eff1-4630-a871-c090e4eed147
DEVICE=eno1
ONBOOT=yes
MASTER=bond0  #新增
SLAVE=yes

#网卡eno2配置
[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-eno2 
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=eno2
UUID=08b2e589-a9e1-4045-8c3f-68608bfffea1
DEVICE=eno2
ONBOOT=yes
MASTER=bond0  #新增
SLAVE=yes

#bond0配置（新创建文件ifcfg-bond0，并添加下边内容）
[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-bond0 
DEVICE=bond0
NAME=bond0
TYPE=Bond
BONDING_MASTER=yes
IPADDR=172.16.3.94
PREFIX=22
GATEWAY=172.16.0.1
DNS1=114.114.114.114
ONBOOT=yes
BOOTPROTO=static
BONDING_OPTS="mode=1 miimon=100"
{% endhighlight %}


6) 重启网卡
{% highlight string %}
systemctl restart network
{% endhighlight %}

7) 查看bond0网卡状态
{% highlight string %}
[root@localhost ~]# cat /proc/net/bonding/bond0 
Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)

Bonding Mode: fault-tolerance (active-backup)
Primary Slave: None
Currently Active Slave: eno1
MII Status: up
MII Polling Interval (ms): 100
Up Delay (ms): 0
Down Delay (ms): 0

Slave Interface: eno1
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 3c:ec:ef:0e:29:0e
Slave queue ID: 0

Slave Interface: eno2
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 3c:ec:ef:0e:29:0f
Slave queue ID: 0
{% endhighlight %}



8)  查看网卡信息ifconfig
{% highlight string %}
[root@localhost ~]# ifconfig
bond0: flags=5187<UP,BROADCAST,RUNNING,MASTER,MULTICAST>  mtu 1500
        inet 172.16.3.94  netmask 255.255.252.0  broadcast 172.16.3.255
        inet6 fe80::3eec:efff:fe0e:290e  prefixlen 64  scopeid 0x20<link>
        ether 3c:ec:ef:0e:29:0e  txqueuelen 1000  (Ethernet)
        RX packets 44110  bytes 2694349 (2.5 MiB)
        RX errors 0  dropped 745  overruns 0  frame 0
        TX packets 542  bytes 49817 (48.6 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255
        ether 02:42:6c:de:c4:73  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eno1: flags=6211<UP,BROADCAST,RUNNING,SLAVE,MULTICAST>  mtu 1500
        ether 3c:ec:ef:0e:29:0e  txqueuelen 1000  (Ethernet)
        RX packets 22250  bytes 1372167 (1.3 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 542  bytes 49817 (48.6 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eno2: flags=6211<UP,BROADCAST,RUNNING,SLAVE,MULTICAST>  mtu 1500
        ether 3c:ec:ef:0e:29:0e  txqueuelen 1000  (Ethernet)
        RX packets 21860  bytes 1322182 (1.2 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 3166622  bytes 771384383 (735.6 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 3166622  bytes 771384383 (735.6 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
{% endhighlight %}


<br />
<br />

**[参看]**




<br />
<br />
<br />


